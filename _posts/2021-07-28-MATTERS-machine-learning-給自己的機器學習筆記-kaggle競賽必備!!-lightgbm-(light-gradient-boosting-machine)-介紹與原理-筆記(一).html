---
layout: post
title: Machine Learning - 給自己的機器學習筆記 - Kaggle競賽必備!! - LightGBM (Light Gradient Boosting
  Machine) - 介紹與原理 - 筆記(一)
date: 2021-07-28 11:42:42.000000000 +00:00
link: https://matters.news/@CHWang/machine-learning-%E7%B5%A6%E8%87%AA%E5%B7%B1%E7%9A%84%E6%A9%9F%E5%99%A8%E5%AD%B8%E7%BF%92%E7%AD%86%E8%A8%98-kaggle%E7%AB%B6%E8%B3%BD%E5%BF%85%E5%82%99-light-gbm-light-gradient-boosting-machine-%E4%BB%8B%E7%B4%B9%E8%88%87%E5%8E%9F%E7%90%86-%E7%AD%86%E8%A8%98-%E4%B8%80-bafyreidrau4x4rcp6gsr7wllksaxtl6gcbgkdvldzf6aky5hv4t2en7ewi
categories: matters
tags: blog
author: 為自己Coding
---

<h2><a href="https://github.com/chwang12341/Machine-Learning/tree/master/LightGBM/Part1" target="_blank">Github連結</a></h2><figure class="image"><img src="https://assets.matters.news/embed/3b496425-4e0b-4953-8631-5eab21d1950c.jpeg" data-asset-id="3b496425-4e0b-4953-8631-5eab21d1950c" referrerpolicy="no-referrer"><figcaption><span>攝影師：Quintin Gellar，連結：Pexels</span></figcaption></figure><h2><br></h2><p><br></p><p><br></p><h2><strong>1. LightGBM是什麼?</strong></h2><p><br></p><ul><li>全名Light Gradient Boosting Machine</li><li>由<strong>微軟</strong>公司於2017年四月釋出的</li><li>為一款基於決策樹(Decision Tree)學習算法的梯度提升框架</li><li>具有快速、分布式和高性能的特性</li></ul><p>在論文中，作者說明他們是如何開發出LightGBM的方法的，他們在傳統的演算法 - Gradient Boosting Decision Tree(GBDT)上添加他們提出的Gradient-based One-Side Sampling (GOSS) 和 Exclusive Feature Bunding (EFB)方法，就成為了LightGBM</p><p><br></p><p><strong>補充:</strong></p><ul><li>Gradient-based One-Side Sampling (GOSS): 為一種有選擇性的取樣，方法只取樣梯度(gradient) 較大的數據點來計算information gain,並省略掉其他的數據點，由於gradient較大的數據對計算 information gain有較多的影響力，所以就省略掉梯度較小的數據點，而這也被作者證明不太會對準確度有影響</li><li>Exclusive Feature Bundling (EFB): 绑定互斥的特徵(Mutually Exclusive Feature)來降低維度，作者表示NP-Hard Problem為直接找出最佳化绑定的方式，但是Greedy算法可以獲得準確度接近的近似解</li></ul><p>Mutually Exclusive Feature: 兩個很少同時採用非零值的特徵(two features that rarely take nonzero values simultaneously)</p><h2><br></h2><h2><br></h2><h2><strong>2. Boosting演算法的發展歷史 - XGBoost、LightGBM、CatBoost</strong></h2><p><br></p><p><strong>為什麼Boosting演算法那麼熱門?</strong></p><p><br></p><p>他們對於訓練資料有限、訓練時間少、專業知識少的引數調優系統中，還是能發揮極大的作用</p><ul><li>XGBoost最早是在2014年由陳天奇提出的一個研究專案</li><li>Microsoft在2017年1月釋出了LightGBM(稳定版本)</li><li>同年2017年四月，俄羅斯的Yandex科技公司(一家在俄羅斯領先的科技公司）開源了CatBoost</li></ul><figure class="image"><img src="https://assets.matters.news/embed/76a9fa51-fe50-4b8c-89cf-ce1bb1b856b9.png" data-asset-id="76a9fa51-fe50-4b8c-89cf-ce1bb1b856b9" referrerpolicy="no-referrer"><figcaption><span></span></figcaption></figure><p><br></p><p>圖片來源: <a href="https://towardsdatascience.com/catboost-vs-light-gbm-vs-xgboost-5f93620723db" target="_blank">https://towardsdatascience.com/catboost-vs-light-gbm-vs-xgboost-5f93620723db</a></p><h2><br></h2><h2><br></h2><h2><strong>3. LightGBMXGBoost?</strong></h2><p><br></p><figure class="image"><img src="https://assets.matters.news/embed/e80c6828-2f08-4aa3-abc4-f8fc88a435a0.png" data-asset-id="e80c6828-2f08-4aa3-abc4-f8fc88a435a0" referrerpolicy="no-referrer"><figcaption><span></span></figcaption></figure><p><br></p><p><strong>XGBoost</strong></p><ul><li>一層一層的往下分裂(如圖)</li><li>XGBoost樹以水平方向生長,它所生長的是樹的層次</li><li>XGBoost採取預分類演算法(presorted algorithm)和基於直方圖的演算法來運算出最佳分割</li></ul><p><strong>LightGBM</strong></p><ul><li>由葉子(leaf)的方向分裂(如圖) - 使用對增益最大的節點進行更進一步的分解方式，這樣可以省下大量分解所耗的資源</li><li>LightGBM樹以垂直方向生長，它所生長的是樹的葉子</li><li>LightGBM挑選具有最大誤差的樹葉往下生長，如果生長一樣的樹葉量，生長葉子的方法可以比用層的方法減少更多的loss</li><li>基於梯度的單側採樣(GOSS)技術來過濾資料例項，來搜尋分割值</li><li>GOSS的方法: 簡單來說就是保留擁有最大梯度的資料，並在其他具有小梯度的例項上採用隨機取樣</li></ul><p>舉例: 假設我手邊有1萬筆資料，其中的2千筆有比較高的梯度，所以被我都保留，剩下的8千筆，隨機選擇個20%，那最後在發現的分割值基礎上，我們選擇了全部數據1萬筆裡面的3千600筆數據</p><ul><li>執行GOSS的時候為了保持相同的資料分布，在計算資訊增益的時候，GOSS提供了小梯度的數據一個常數乘數，所以在減少資料例項的數量和保持學習決策樹的準確度上取得了非常好的平衡</li></ul><p><br></p><p>我覺得這篇:  <a href="https://codertw.com/%E7%A8%8B%E5%BC%8F%E8%AA%9E%E8%A8%80/510420/" target="_blank">https://codertw.com/程式語言/510420/</a>  寫得非常非常厲害,它幫我們詳細比較了XGBoost、LightGBM 和CatBoost，也清楚寫出方法的過程原理，推薦大家閱讀</p><h2><br></h2><p><br></p><h2><strong>4. 為什麼使用LightGBM這麼熱門?</strong></h2><p><br></p><p>由於它具有以下的優勢，讓它迅速的被廣泛使用</p><ul><li>名稱中有個Light，表示它有很快的訓練速度</li><li>占用很少的內存空間</li><li>支援並行的學習方式</li><li>它可以用來處理大量的數據</li><li>還可以支援GPU學習</li><li>擁有更高的準確率</li></ul><h2><br></h2><h2><br></h2><h2><strong>5. 任何情況都適用LightGBM嗎?</strong></h2><p><br></p><p>當然不是囉，LightGBM對於過擬和(overfitting)很敏感，所以不適合用於小型數據集，對於小型數據集非常容易overfitting</p><h2><br></h2><h2><br></h2><h2><strong>Reference</strong></h2><p><a href="https://en.wikipedia.org/wiki/LightGBM" target="_blank">https://en.wikipedia.org/wiki/LightGBM</a></p><p>論文： 1999 REITZ LECTURE GREEDY FUNCTION APPROXIMATION: A GRADIENT BOOSTING MACHINE1 By Jerome H. Friedman</p><p><a href="https://zhuanlan.zhihu.com/p/52583923" target="_blank">https://zhuanlan.zhihu.com/p/52583923</a></p><p><a href="https://kknews.cc/zh-tw/tech/y3a3x8j.html" target="_blank">https://kknews.cc/zh-tw/tech/y3a3x8j.html</a></p><p><a href="https://github.com/denistanjingyu/LTA-Mobility-Sensing-Project" target="_blank">https://github.com/denistanjingyu/LTA-Mobility-Sensing-Project</a></p><p><a href="https://codertw.com/" target="_blank">https://codertw.com/</a>程式語言/510420/</p><p><a href="https://lightgbm.readthedocs.io/en/latest/Parameters.html" target="_blank">https://lightgbm.readthedocs.io/en/latest/Parameters.html</a> <a href="https://towardsdatascience.com/catboost-vs-light-gbm-vs-xgboost-5f93620723db" target="_blank">https://towardsdatascience.com/catboost-vs-light-gbm-vs-xgboost-5f93620723db</a></p>
