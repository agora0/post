---
layout: post
title: 人工智能冒起　是創新科技與生活智慧？　還是自招滅亡的生存威脅？
date: 2021-06-12 10:01:58.000000000 +00:00
link: https://thestandnews.com/technology/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%E5%86%92%E8%B5%B7-%E6%98%AF%E5%89%B5%E6%96%B0%E7%A7%91%E6%8A%80%E8%88%87%E7%94%9F%E6%B4%BB%E6%99%BA%E6%85%A7-%E9%82%84%E6%98%AF%E8%87%AA%E6%8B%9B%E6%BB%85%E4%BA%A1%E7%9A%84%E7%94%9F%E5%AD%98%E5%A8%81%E8%84%85/
categories: stand
tags: blog
author: Edward Ho
---

<img width="600" src="https://images.weserv.nl/?url=cdn.thestandnews.com/media/photos/cache/20210612-11_Txyah_600x0.png" /><br /><br /><p>近年多國政府、科技巨頭專注研究人工智能（AI），一方面為生活、營商帶來新習慣及新機會，同時亦衍生出歧視不公、道德倫理問題。有指人工智能技術被政權用作操控工具：其中最受國際關注的，正是中國政府被指利用 AI 技術，監控新疆維吾爾族與其他少數族裔。人工智能技術每日躍進，是帶來更美好的高智慧生活，抑或是人類自招滅亡的生存威脅？</p>
<p>大眾對於人工智能所帶來的風險或威脅，很大機會建基於科幻小說、電影的想像；無疑，人工智能科技已可應用於武器科技上，成為「殺人工具」，這已非小說情節，聯合國上周發表的報告亦指出，<a href="https://www.livescience.com/ai-drone-attack-libya.htm" target="_blank">至少有一部自動控制無人機用於擊殺軍人</a>，但未知由誰人操控—這則新聞印證了 2017 年學者預計的問題，加州大學人工智能教授 Stuart Russell 曾與其他學者及倡議人士發起聯署，<a href="https://beta.thestandnews.com/technology/%E5%AD%B8%E8%80%85%E5%80%A1%E7%9B%A1%E5%BF%AB%E7%A6%81%E6%AD%A2%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%E6%AD%A6%E5%99%A8" target="_blank">要求各國禁止研發人工智能武器</a>。</p>
<p><strong>「情緒辨識」功能準確性成疑</strong></p>
<p>不過人工智能帶來的問題並不僅如此，同樣也會被政權用於人口監控，或視為定罪手段，情況令人擔憂。美國科技巨頭之一的 IBM 去年六月就宣布，停止人臉辨識技術研究計劃，當時 IBM 行政總裁 Arvind Krishna 明言，<a href="https://beta.thestandnews.com/international/%E6%86%82%E5%9F%B7%E6%B3%95%E9%83%A8%E9%96%80%E6%BF%AB%E7%94%A8-ibm-amazon-%E7%9B%B8%E7%B9%BC%E5%AE%A3%E4%BD%88%E5%81%9C%E6%AD%A2%E6%88%96%E6%9A%AB%E7%B7%A9%E4%BA%BA%E8%87%89%E8%BE%A8%E8%AD%98%E7%B3%BB%E7%B5%B1%E9%96%8B%E7%99%BC" target="_blank">憂慮技術會被執法部門使用</a>。</p>
<p>去年底，<a href="https://beta.thestandnews.com/international/%E8%A2%AB%E6%8F%AD%E4%BA%BA%E8%87%89%E8%AD%98%E5%88%A5%E7%B3%BB%E7%B5%B1-%E5%8F%AF%E8%BE%A8%E8%AD%98%E7%B6%AD%E5%90%BE%E7%88%BE%E4%BA%BA-%E9%98%BF%E9%87%8C%E5%B7%B4%E5%B7%B4-%E5%B7%B2%E5%88%AA%E9%99%A4%E7%A8%AE%E6%97%8F%E6%A8%99%E8%A8%98%E5%8A%9F%E8%83%BD" target="_blank">影象監控研究組織 IPVM</a> 亦發現阿里巴巴、 Kingsoft 、華為等公司，都有開發可用作辨識新疆維吾爾族人的人臉辨識系統；上月底則有中國軟件工程師對英國廣播公司透露，<a href="https://beta.thestandnews.com/china/bbc-%E6%96%B0%E7%96%86%E8%AD%A6%E5%B1%80%E5%AE%89%E8%A3%9D%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%E5%81%B5%E6%B8%AC%E7%B6%AD%E5%90%BE%E7%88%BE%E4%BA%BA%E6%83%85%E7%B7%92" target="_blank">新疆警察部門正測試人工智能情緒辨識功能</a>，甚至以此視為定罪證據之一。然而這類系統準確性成疑，或會被政權利用，成為將異見人士定罪的工具。</p>
<p>另一例子是中國科技龍頭騰訊研發的「智能檯燈」、裝有鏡頭，產品雖然無聲稱有「情緒觀察功能」，<a href="https://zhuanlan.zhihu.com/p/354859592" target="_blank">但可用於矯正坐姿及批改作業等</a>。香港以及外國也有一些科技公司嘗試開發類似系統，稱可以<a href="https://www.4littletrees.com/" target="_blank">辨別兒童</a>或者是駕駛者的情緒。</p>
<p>《衞報》訪問即將發佈新書 Atlas of AI 的微軟研究人員 Kate Crowford ，被問及相關情緒辨識功能時， Crowford 直指，以臉容辨識情緒的工具有「嚴重缺陷」，並認為這是「不可能」，認為相關系統是最急需要規管的範疇。</p>
<p><a href="https://www.nature.com/articles/d41586-021-00868-5" target="_blank">Crowford 亦曾在《自然》發表論文</a>，倡議要進一步規管人臉或情緒辨識這類研究。她指，這類規管可以預防「顱相衝動（phrenological impulse）現象」—即僅透過外在容貌，對一個人的內在狀態、能力作出錯誤假設，實際目的是提取其個人信息。</p>
<p>Crowford 解釋，現時大部份情緒辨識系統都是基於 1970 年代、以心理學家 Paul Ekman 為主的研究，當時的心理學研究提出可用適當工具「讀心」。然而，近年有更多心理學研究已推翻此類說法，並指表情並不能直接反映到一個人的內心情緒。</p>
<p>「然而，我們卻有科技公司聲稱可以透過一個人的臉部影片推斷出情緒，甚至將其置於<a href="https://www.vice.com/en/article/m7jpmp/car-companies-want-to-monitor-your-every-move-with-emotion-detecting-ai" target="_blank">汽車系統內。</a>」在欠缺科學實證下，此類工具更易被用作打壓異見人士的工具，人權監察中國區總監 Sophie Richardson 曾對 BBC 表示：「<a href="https://beta.thestandnews.com/china/bbc-%E6%96%B0%E7%96%86%E8%AD%A6%E5%B1%80%E5%AE%89%E8%A3%9D%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%E5%81%B5%E6%B8%AC%E7%B6%AD%E5%90%BE%E7%88%BE%E4%BA%BA%E6%83%85%E7%B7%92" target="_blank">⋯⋯處於高度脅迫條件、高壓、在可被理解的緊張情緒下，被視作為有罪的指標，我認為這是非常有問題。</a>」</p>
<p><strong>或衍生更多歧視不公</strong></p>
<p>人工智能系統亦衍生出歧視與不公。人工智能系統要成功運作，需要事前作出大量訓練，最常用的訓練方法，就是讓系統透過大量數據「學習」到一些模式：比如腳較長的動物應該是「馬」、腳較短的動物有機會是「狗」等，然而這類系統亦出現一個問題，就是會因不同人的膚色出現具偏見的判斷，例如黑人或更易被認作為罪犯等。</p>
<p>有部份人提出，採集更多數據可解決相關問題，但 Crowford 認為，問題不僅是出自訓練用的數據上，更大的問題在於系統訓練方法。她說：「用於機器學習軟件的訓練數據庫會隨意將人分為兩種性別，可以根據膚色，將人們標記為五個種族類別，同樣也會試圖根據人們的外觀，判斷一個人的道德或倫理品格。」</p>
<p>她又指，根據外觀做出這些決定的假設，本身已擁有一段「黑歷史」 ，「不幸的是，這些分類政治問題，已深深刻劃在人工智能的基礎上。」</p>
<p>Crowford 續以一套常用於訓練人工智能系統、擁有超過 1,400 萬張圖片的圖片數據庫 ImageNet 作為例子；圖片庫曾被 Crowford 與另一學者發現，用作標記的「分類字眼」極具爭議性，Crowford 形容這些分類字眼「充滿著厭惡女性、種族主義、健全主義（ableist） 、並具極端判斷性」。</p>
<p>舉例而言，人們的照片會被標記為盜竊狂、酒鬼、壞人、壁櫥女王、應召女郎、蕩婦、吸毒者，甚至是一些她難以說出口的字眼。她指出，雖然 ImageNet 現時已改善其分類，但有很多私人公司採用的機密訓練數據庫，可能同樣存有這類問題。</p>
<p>「我們應該要小心應對人工智能，如果要我去推斷我們最大的生存威脅的話，那應該就是它了。」 <a href="https://theconversation.com/elon-musk-is-right-we-need-to-talk-about-artificial-intelligence-33577" target="_blank">SpaceX 創辦人 Elon Musk 年前出席麻省理工學院座談會時曾這樣說</a>，足見人工智能有機會帶來的問題。</p>
<p>Crowford 就指，倫理問題固然重要，但更重要是哪些人的利益會因系統受惠、哪些人的利益會因而受損，系統會否令當權者擁有更多權。她認為，現實已一次又一次反映類似系統「壯大」了當權者，例如企業、軍方以及警察的力量。</p>
<p>人工智能科技近年雖不斷躍進，但與所有新興科技一樣會帶來更多問題。Crowford 表示，她仍對此範疇感到希望，「今年 4 月，歐盟制定了第一份人工智能綜合法規草案；澳洲也剛剛發表了監管人工智能的新指南。無疑我們需要修補一些漏洞——但我們現在已開始意識到，這些工具需要用上更堅固的『護欄』。」</p>
<p>來源：<br/>
The Guardian, <a href="https://www.theguardian.com/technology/2021/jun/06/microsofts-kate-crawford-ai-is-neither-artificial-nor-intelligent?fbclid=IwAR3C2UV9viDSJyxRcqiXw2Nm0cs4wHbLrlKatU3CvRdgaHtYFl_6uC7PG_4" target="_blank">Microsoft’s Kate Crawford: ‘AI is neither artificial nor intelligent’</a>, 6 June 2021</p>
