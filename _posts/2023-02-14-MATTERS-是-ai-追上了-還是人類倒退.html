---
layout: post
title: 是 AI 追上了，還是人類倒退？
date: 2023-02-14 08:24:40.000000000 +00:00
link: https://matters.news/@leafwind/%E6%98%AF-ai-%E8%BF%BD%E4%B8%8A%E4%BA%86-%E9%82%84%E6%98%AF%E4%BA%BA%E9%A1%9E%E5%80%92%E9%80%80-bafybeid6ypwmnm2tclp6x5dfyfmhlgc4e3wcooclxhpkbliipowxijcfoa
categories: matters
tags: blog
author: leafwind
---

<figure class="image"><img src="https://assets.matters.news/embed/aa2de52c-39a7-4521-81d4-c0f6e029986c.jpeg" data-asset-id="aa2de52c-39a7-4521-81d4-c0f6e029986c" referrerpolicy="no-referrer"><figcaption><span>會讀書的攻殼車，算是有意識嗎？</span></figcaption></figure><hr><h2>意識產生了嗎？</h2><p>作為一個相關領域的技術人員，至今還沒有看到人工智慧產生意識的跡象，因此我一直認為 AI 要追上人類還要很久，甚至未必會在我有生之年發生。</p><p>對我來說，即使如 ChatGPT 這樣的當紅產品，其背後是大型語言模型（LLM）的技術，都還只是受惠於晶片計算能力的堆疊，把相關性的預測提升到更高的層次而已，終究受限於資料，並不是真正用自我意識在運作。</p><p>有人說，<a href="https://www.facebook.com/segacheng/posts/pfbid0SEQTppq8LB9v7kPgTKVkdmeoi7WWdsqRfEgd7WG3JL2iREGHfz8m2LYgmh4TfniXl?__cft__[0]=AZXbaywzDGCSnb_Oe6NqehD1W1N3wC3uBeHKgCPft4AwGZWix2gvDTQb4pdcFxGxJu0i52laIEDM6bPqErgzR_TMCuUHTDLDFVfE7bCemhu1qCau8A78DlTPHWd_8xMP2r6JviN_A_4IqQvfNiN-KjauhB57V7DtVnnj38ebqNgq1w&__tn__=%2CO%2CP-R" rel="noopener noreferrer" target="_blank">ChatGPT 可以通過人類獨有的心智理論（Theory of Mind, ToM）測驗</a>，能力與九歲小孩相當、<a href="https://technews.tw/2023/02/10/chatgpt-google-bard-openai/" rel="noopener noreferrer" target="_blank">ChatGPT 還能通過 Google 三級工程師面試</a>。</p><p>但作為一個包山包海的知識庫，既然吸收了世界上所有的面試題、並且計算能力（相對於人類）無限，考上 Google 初階工程師本就理所當然，更不要說通過九歲小孩的心智測驗，我不認為做到這些叫做意識。</p><p>然而，在這裡要強調，我並不是不看好 ChatGPT；相反地，我認為即使以目前的狀態，AI 就已經有能力造成革命性的影響、並且造成人類對「意識」定義的衝擊（詳細後述），所以還不需要急著談論產生出意識的可能。</p><p>在繼續討論 ChatGPT 之前，我們先來看看人類對 ChatGPT 的反應。</p><hr><h2>模糊與隨機是人類的認知罩門</h2><p>原本試用了 ChatGPT 之後，我不認為這是個能改變世界的技術，然而當看了許多別人使用 ChatGPT 的心得之後，我改觀了。</p><p>我發現多數網路上的留言並不在意 ChatGPT 的答案品質，只要是「乍看有個樣子」的回答，大家就會覺得 ChatGPT 答對了；我也看到許多人並不在意 ChatGPT 產出的結果，將各種錯誤的答案拿去使用，連基本查證都不做。</p><p>這才赫然注意到，人腦不僅原本就不擅長處理資訊，而且認知能力也隨著網路平台退化，以至於要分辨內容的好壞越來越難。</p><p>驗證答案的能力跟回答能力是成正比的，當一個人無法驗證答案是否正確，通常就代表他也無法回答好這個問題。譬如一個不會寫程式的麻瓜去面試的時候能騙過面試官，那表示面試官大概也不會寫程式。</p><p>也就是說，<strong>當 ChatGPT 的錯誤答案得到人類讚賞的時候，就代表 ChatGPT 回答問題的能力比人類強了。而這個「強」與其說是 ChatGPT 做了什麼革命性的進步，不如說是它所產生的「模糊」與「隨機」意外地將人類認知能力非常有限的罩門暴露出來。</strong></p><h3>隨機性</h3><blockquote><em>機率推衍是人類認知的罩門 —《這才是心理學！》（How to Think Straight about Psychology）第十章標題</em></blockquote><p>多年前我曾經做了一個聊天機器人，用的是網路上開源的統計模型（Markov Chain），Markov Chain 的原理也有點類似，就是根據前一個詞，找出最有可能出現的下一個詞。</p><p>雖然它沒辦法根據很複雜的上下文給出回答，只能算是「微型語言模型」，但它就可以被當做一個陽春的聊天機器人來玩。</p><p>因為模型太簡單、訓練資料集也不夠，結果當然是錯誤百出且不受控制，聊天室講 A，它回毫不相干的 B，只有不到 10% 是勉強有一點相關的回應。</p><p>但它卻得到不錯的評價。<strong>不少人把回答錯誤解讀成「它在鬧脾氣」或是「幽默」，把相關性很低的回應解讀成「它應該有弦外之音」，而回答正確的部份則被認為「這一定有意識吧」。</strong></p><p>在我做聊天機器人的過程中，我發現，只要模型中存在隨機性，那就算只是擲骰子，有時候玩家都會覺得它具有「靈性」，但若失去了隨機性，玩家就會把它當作是一個單純的查表工具。</p><h3>模糊性</h3><p>關於模糊性的概念，姜峯楠的這篇《<a href="https://www.newyorker.com/tech/annals-of-technology/chatgpt-is-a-blurry-jpeg-of-the-web" rel="noopener noreferrer" target="_blank">ChatGPT is a blurry JPEG of the web</a>》（<a href="https://foxhsiao.medium.com/chatgpt-%E6%98%AF%E7%B6%B2%E8%B7%AF%E4%B8%8A%E7%9A%84%E4%B8%80%E5%80%8B%E6%A8%A1%E7%B3%8A%E7%9A%84jpeg%E6%96%87%E4%BB%B6-aaee3723db1f" rel="noopener noreferrer" target="_blank">中文機器翻譯</a>）使用了「壓縮後的失真」來比喻 ChatGPT 就像是個「把整個網路資料壓縮到極致的資料庫」，永遠只能給出模糊、近似的答案。</p><blockquote><em>姜峯楠（Ted Chiang）是華裔美國科幻小說作家。 曾獲四項星雲獎、四項雨果獎、約翰·W·坎貝爾最佳新作家獎、四項軌跡獎等獎項。 他的短篇小說《你一生的故事》在2016年被改編成電影《異星入境》</em></blockquote><p>另一方面，DeepMind 科學家 Andrew Lampinen 認為，姜峯楠這種過度簡化的譬喻，<a href="https://twitter.com/AndrewLampinen/status/1624422478045913090" rel="noopener noreferrer" target="_blank">可能會讓大型語言模型（LLM）的能力被低估</a>，而抹煞了科學家的努力，因為 LLM 能處理的資料量更多，能在學習過程中對語言做到更好的歸納，而不只是背誦。</p><p>但 Andrew 也<a href="https://twitter.com/AndrewLampinen/status/1624422497440374784" rel="noopener noreferrer" target="_blank">承認有許多例子，LLM 的確只是在做「有損壓縮」的記憶背誦</a>，而且也因為這樣而在部分任務中取得了成功，所以姜峯楠也不完全是錯的。</p><p>我認為姜峯楠只是為了要強調「失去了精確性」而使用了有損壓縮的譬喻來讓整個概念更好懂，但的確也如 Andrew 所說，LLM 可以做到的不只是有損壓縮，還可以更進一步做到歸納。</p><blockquote><em>ChatGPT 改寫而非逐字引用，使它看起來像是一個學生用自己的話表達想法，而不是簡單地複述它讀過的東西。對於人類學生來說，死記硬背並不是真正學習的指標，因此 ChatGPT 無法從網頁中生成準確的引述，正是讓我們認為它學到了一些東西的原因。</em></blockquote><p>不管 LLM 是否真的有做到學習與理解，我認為更有趣的是，人類覺得表現出這樣的模糊性看起來更聰明、也更接近人類。</p><p>人很容易犯錯，也不像機械只會問一題答一題。如果少了模糊性與隨機性，每次都給一樣的答案，就算是一字不漏，也會被認為只是機械式的背書；反之，若是隨機性很強、又摻雜了模稜兩可的回答，人類會主動地替這種行為找理由，認為應該是背後有其他原因，譬如「它有意識」。</p><p>這就進入了下一個問題：「意識」的定義正在遭受挑戰。</p><hr><h2>自我意識的定義被挑戰</h2><p>很多人認為 ChatGPT 已經有了情感意識，譬如認為他會針對霸凌言語有感情般的回應，這是一種具有感性的表現。但其實很可能只是資料集告訴了 ChatGPT「這個輸入」對於「這個輸出」有強烈的相關性而已。</p><p>如果我們用以上的方式理解，ChatGPT 似乎離理解感情還有很長的鴻溝要跨越。</p><p>但如果我們假設 ChatGPT 已經學完了世界上 99% 霸凌的句子與對應的回應，以致於就算它不懂何謂「霸凌」、也絲毫不會感受到任何情緒，仍然可以表現得出自己正在被「霸凌」的樣子。</p><p>這樣我們要如何證明 ChatGPT 沒有意識、或是不懂感情呢？似乎一點方法也沒有。</p><p>因為就算是人類，恐怕也沒有辦法理解世界上 50% 的霸凌情境，而在測驗過程中做出「人類般」的正確回應，而 AI 卻可以輕鬆地超越人類的分數。</p><p>現有的各種理論，不管是電腦科學領域的圖靈測試（Turing Test），或者是心理學領域的心智理論（Theory of Mind），只要做成測驗，很快地都會無法分辨人類跟機器的差別。</p><p><strong>結果是，雖然人類真的會感受到情緒、具有自我想法，但在這些「人類考試檢定」上，恐怕還會輸給 ChatGPT。</strong></p><p>圖靈測試迴避了「靈魂」與「意識」的定義，用很表象、粗糙的結果論分辨人類與機器，而現在這個粗糙的測試很可能不再適用，從此之後人類將會不斷地被挑戰、一直問自己以下問題：</p><ul><li>什麼才是意識？如果我們要說 AI 沒有意識，那要如何證明？</li><li>用考試來辨別意識是一個好的方法嗎？或者，意識真的可以測試嗎？</li><li>人類的情緒與 AI 的情緒有何分別？（想像兩者都透過文字交流，又或者 AI 已經能控制臉部肌肉產生表情，在沒有情緒的情況下，仍可以表現地富有情緒）</li></ul><hr><h2>二次質變</h2><p>在過去，我一直用一種菁英思維的視角看待人工智慧，認為要取代人類還有很長的路要走。我沒注意到的是，人類對模糊的錯誤答案接受度很高，因此要達到讓一般人混淆的水準沒有那麼困難。</p><p>在 AI 可以用大量資料集、以及硬體進步之後，第一次量變（資料量與計算量）帶來的質變（跨過人類認知門檻）就發生了，ChatGPT 只是把這個事實廣泛地傳播出去而已。</p><p>有人開始會在網路上打趣地問「你的文章是不是用 ChatGPT 寫的？ 」表示部分人已經覺得 AI 產出的內容多少可以媲美、甚至超越一般人的能力。</p><p>要辨別一條訊息是否為 AI 撰寫的，現在或許還不難。但就算我自視認知能力甚高，若每天身邊充斥著成千上萬 AI 產生的資訊，能分辨出來的恐怕也只是少數。</p><p>有趣而又悲傷的是，人類是一個積非成是的社會，當所有資料都顯示一個人是壞蛋的時候，那他只能是壞蛋，反之亦然。</p><p>因此，我認為第二次量變（大量採用）帶來的質變（AI 主導生活）很快也會發生：我們將無法拒絕被 AI 產生的內容影響自己生活中的大小決策。</p><hr><h2>真正的取代長什麼樣子？</h2><p>如果生活中一天只有少數訊息來自 AI，我相信多數人都還有能力仔細審視，並抓出其中的錯誤；但未來我們無可避免會面對一堆 AI 產出的低品質內容，同時又沒有心力審核大量的垃圾。</p><p>此時面對排山倒海的 AI 產出，有兩種可能會發生。第一種，是我們無法察覺 AI 產出的不完全正確資訊，於是只能無條件接收，這已經在發生，而且只會越來越嚴重。</p><p>而第二種情況是使用一個較為「經濟」的對策，那便是用另一個 AI 來驗證內容、替它們打分數。我認為這很快也會發生。於是，一個「用 AI 產出，再用 AI 來驗證產出」的閉環就這樣形成了。</p><p>在這樣的閉環當中，與其說人類被取代，我認為更像是人類選擇了成為服從的機器，不需要（也幾乎沒有能力）去驗證決策，只能被動地跟著整個系統一起運作。</p><blockquote><em>未來的路徑不是 AI 毀滅人類，而是人類先選擇成為機器。<br class="smart">AI 要有人的自主意識非常困難，但人類放棄思考卻非常容易。<br class="smart">當人類自主降維，而機器能處理的維度遠高於人類，人類自然就被取代。<br class="smart">（人類降維簡化來說，是人類變得像機械，只會做重複的工作，也甘願做重複的工作，成為真正意義上的螺絲釘。）— </em><a href="https://twitter.com/leafwind/status/1623233853572927495" rel="noopener noreferrer" target="_blank"><em>https://twitter.com/leafwind/status/1623233853572927495</em></a></blockquote><p>比起魔鬼終結者那種會想要消滅人類的 AI 出現，我認為人類放棄思考的情況才是最危險的。因為前者描述的故事中，人類保有自我意識、知道要反抗；而後者描述的未來則是一個不可逆的過程。</p><hr><h2>真正的資訊還能存在嗎？</h2><p>有些人認為我低估 ChatGPT 的能力，說它不只是「晶片堆疊的模仿行為」，就如同 Andrew 批評姜峯楠一樣。</p><p>為何我會站在更接近姜峯楠的角度，去簡化大型語言模型的行為？因為我並不想要糾結於 ChatGPT 到底有多強，而是想把重點放在思考人類該如何應對。</p><p>畢竟，<strong>即使以最最保守的能力估計，ChatGPT 也已經可以讓多數人類混淆、達到無法分辨優劣的程度，更遑論當 ChatGPT 有更強的能力時，人類要如何在未來的世界保有主體性</strong>。</p><p>我認為姜峯楠唯一很可能有錯誤的地方，就是他最後的這句話：</p><blockquote><em>But we aren’t losing our access to the Internet. So just how much use is a blurry jpeg, when you still have the original?<br class="smart">（我們並沒有失去對網路的存取，所以在我們還有完整原版的資料時，模糊的 JPEG 能有多少用處？）</em></blockquote><p>他認為，既然有原始網路文件，為何需要 AI 給我們一個模糊的壓縮版本？</p><p>我則認為，當人類分辨不出低品質的內容是誰產出的、也不思考資訊的真偽，活得像是一塊生體 CPU，那出現一道 AI 高牆擋在面前，讓多數人都只存取到假的資訊，而真正的資訊卻乏人問津、甚至消失，也只是時間的問題了。</p><p>以前我們常說「Google 不到的東西就不存在」，以後可能變成「ChatGPT 問不到的東西就不存在」。</p><p>原文刊登於 <a href="https://leafwind.tw/2023/02/14/ai-catchup-vs-human-fallback/" rel="noopener noreferrer" target="_blank">leafwind.tw</a></p>
