---
layout: post
title: ChatGPT要取代传媒了吗？端编辑室的一场“人工智能”小实验｜工具人
date: 2023-03-08 15:00:00.000000000 +00:00
link: https://theinitium.com/article/20230309-the-techsmiths-is-ai-replacing-journalists/
categories: initium
tags: blog
author: 陈婉容
---

<p>「当你预测一种新的技术工具要开创一个新时代时，问问自己：真的会吗？」</p><figure class="image" itemscope itemtype="https://schema.org/ImageObject"><a class="image" href="https://theinitium.com/article/20230309-the-techsmiths-is-ai-replacing-journalists/undefined"><img src="https://d32kak7w9u5ewj.cloudfront.net/media/image/2023/03/41ee8e9db0ba45bc880a61f4f4a9af77.jpeg?imageView2/1/w/1080/h/720/format/jpg" alt="ChatGPT 设计图片。" itemprop="contentUrl" referrerpolicy="no-referrer"></a>
            <figcaption itemprop="caption">ChatGPT 设计图片。<span class="credit">摄：Beata Zawrzel/NurPhoto via Getty Images</span></figcaption></figure><p><small>“工具人”是端传媒新开设的栏目。近年我们迎来了一波科技大爆发：人工智能﹑大数据和机械人似乎从科幻般的遥远未来被拉到我们面前，眼看就要对我们的生活造成巨大影响。都说人类站到食物链顶层是因为懂得使用工具，但这些工具会如何倒过来模塑人类社会？栏目不定期刊出探讨科技﹑社会与人文的深度报道，请<a href="https://theinitium.com/channel/the-techsmiths">点击</a>订阅。</small></p>
<h2>ChatGPT要取代你了</h2>
<p>当我建了个叫“ChatGPT要取代你了”的群组，然后把端的各组同事加进群里的时候，有同事语带哀怨地问我：“为甚么我们要测试自己会不会被取代？”我说：“这是个好问题，我会把它写进文章里的。”</p>
<p>为甚么呢？虽然我高度怀疑“人工智能取代人类”的叙事，但ChatGPT一类大型语言模型（LLMs）会“取代”编辑记者的标题实在是铺天盖地。我随便搜了一下新闻：美国科技新闻公司CNET在数星期前被发现自去年11月起，就开始使用人工智能撰写金融类解释性报道。三月初，CNET被爆出大量<a href="https://www.theverge.com/2023/3/2/23622231/cnet-layoffs-ai-articles-seo-red-ventures">削减</a>人手，虽然公司指裁员和在编辑室使用人工智能无关，但不久前收购CNET的财团RedVentures就指“要简化运营和枝术栈（tech stack）”，而CNET的前主编也变成了“人工智能策略副总裁”。同是三月初，Axel Springer（旗下有《政客》（Politico）﹑《商业内幕》（Business Insider）等多份著名报章杂志的德国出版公司）的首席执行官在一份内部通讯里，指ChatGPT一类的人工智能工具会掀起一场革命，又指这些工具“很快会比人类记者更会汇整资讯”。这家公司也发表<a href="https://www.theguardian.com/technology/2023/mar/01/german-publisher-axel-springer-says-journalists-could-be-replaced-by-ai">声明</a>，指旗下《图片报》等报章将会裁员，因为人工智能很快就会将大量员工变成冗员。</p>
<p>这些都只是冰山一角。ChatGPT一类“人工智能工具”（必须加上括号，原因下述）似乎真的对新闻行业构成了某种生存威胁。而这种行业的位置本身又特别尴尬。我想起高中的时候，班主任知道我想要报读新闻系，经常有意无意在我身边说：“当记者很辛苦哦”﹑“我还是觉得港大法律系适合你”。而作为处于Web 2.0时代的Y世代少女，我在自己的Xanga网志里反驳了她无数遍，一阐述起新闻理想就洋洋洒洒五千字。对许多做新闻的人，“被取代”似乎不止是饭碗不保的问题，几乎是在质疑人生最核心的信念。小时候明明说要走遍世界﹑报道真相﹑为弱势发声……这些原来都是机械都可以做的吗？早知道就去当律师算了（虽然，人工智能取代律师的<a href="https://www.reuters.com/legal/transactional/will-chatgpt-make-lawyers-obsolete-hint-be-afraid-2022-12-09/">报道</a>也不在少数）。</p>
<p>在整个新闻行业似乎难以避免被大型语言模型撼动的时候，我召集了端传媒编辑室进行了一场不科学﹑不严谨，但应该有点意义的小实验：我们要试用ChatGPT来进行日常工作，看看我们当中谁可以被有效取代。于是，一群编辑﹑记者和设计师们，就开始了一场“失业式”实验。</p>
 
 
 
 
 
 
 
 
 

 
 
 
 
 
 
 
 
 
 
 
 
 
 
 

 
 
 
 
 
 
 
 
 
 
 
 

 
 
 
 
 
 
 
 
 
 

 
 
 
 
 
 
 
 
 
 
<p>一场不严谨的实验</p><p>我请自称“重度人工智能使用者”，现时在欧洲攻读传播学硕士的实习记者Stacey制作了一份“ChatGPT使用指南”，内容包括如何先把一些文章“喂”给ChatGPT再让它分析﹑如何利用ChatGPT的“对话”特性来逐步将指令精确化，以及如何利用角色扮演来给它指令，让它进行写作等。各组同事会按著这份指南来用ChatGPT进行日常工作：Stacey用ChatGPT帮她做一篇朝鲜网路攻击报道的前期研究﹑社媒组用它写脸书帖子﹑我用它来改稿……等等。</p><p>以下是“实验”的部份结果：</p><p>一：ChatGPT可以去国家宣传部门上班</p><p>我请社媒编辑小刀试试用ChatGPT写我们脸书专页的帖子，如果写得可以又查核过里面的事实没错，还可以直接发掉，我们之后才向读者揭晓哪篇是ChatGPT的手笔。可惜小刀对于ChatGPT的表现很不满意，还问我：“为甚么它写的东西那么像国家宣传部门写的……”</p><p>社媒组让ChatGPT试写一个关于<a href="https://theinitium.com/article/20210723-mainland-wenzhou-bullet-train-crash-10-year/">温州动车事故</a>的脸书帖文。虽然ChatGPT的“知识截止日期”是2021年，但对于2011年发生的温州动车事故它表示“没有足够的信息”。在手动喂了些资料给它，来来回回几次后，它写了这样的一篇帖文：</p><p>
</p><p>“2011年的‘723’温州铁路事故，在中国铁路历史上留下了悲惨的一笔。事故造成了40人的丧失，172人受伤，经济损失高达1.9亿。十二年过去了，事故现场最近的温州市鹿城区下峃村正面临拆迁，旧房被拆除，新楼也在建造。然而，每隔几十分钟就有一列火车疾驶而过，提醒著我们不应忘记当年的事故。让我们继续记住这段不幸的历史，以保持对安全的戒慎，避免再次发生此类惨案。”</p>
<p></p><p>试了几次用ChatGPT写不同的新闻事件帖文后，小刀表示ChatGPT写的东西“真的非常中宣部”，实在用不了：“就是非常的正能量，正向思考，不管你喂给他的词是正面的，负面的，他都给你说的很正面，对很负面的词也会给你一个反应是，我们要学习，我们要避免再次发生，我们不能忘记。一种正面且官腔的说法，而且非常的含情脉脉，就差不是央视主持人对镜头说出来了……”</p><p><a class="image" href="https://theinitium.com/article/20230309-the-techsmiths-is-ai-replacing-journalists/undefined"><img src="https://d32kak7w9u5ewj.cloudfront.net/media/image/2023/03/bc2a9d769f58440abdaaf6ef589c38b9.jpeg?imageView2/1/w/1080/h/720/format/jpg" alt="2022年10月26日，中国上海，市民在告示板上看报纸。" itemprop="contentUrl" referrerpolicy="no-referrer"></a>
</p><figcaption itemprop="caption">2022年10月26日，中国上海，市民在告示板上看报纸。<span class="credit">摄：Qilai Shen/Bloomberg via Getty Images</span></figcaption><p></p><p>二：ChatGPT提供的资讯必须再三查证</p><p>端记者陈阿烟为了写关于国务院新任总理李强的<a href="https://theinitium.com/article/20230307-mainland-who-is-liqiang/">文章</a>，用ChatGPT和Bing ChatGPT查询了一下李强的生平，以及李强和中共其他高层的关系。他问李强生平的时候，ChatGPT说李强和前国务院总理朱镕基关系密切。细问之下，ChatGPT这样说：</p><p>
</p><p>“李强和朱镕基的关系可以追溯到1990年代初期，当时朱镕基任上海市委书记，而李强是他的助手和重要幕僚。朱镕基和李强一起推动了上海的经济发展和市政建设，并在国内外赢得了很高的评价。朱镕基在上海市政府任期结束后，曾多次在公开场合赞扬李强的工作和贡献。”</p>
<p></p><p>至于李强和现任国家主席习近平的关系，ChatGPT这样说：</p><p>
</p><p>“李强与习近平的关系可以追溯到2000年代初期。当时习担任福建省省长，而李强则是福建省省委常委和省政府秘书长。习和李在福建省的工作期间建立了良好的工作关系和私人友谊。”</p>
<p></p><p>阿烟说：“李强没跟朱镕基有关系，真的完全瞎扯。习近平的那段就一句真一句假。习近平2000年在福建当省长是真，李强那时是他的秘书是假。但李强后来的确是他的秘书。李强是在浙江当习近平的手下，不是福建。”</p><p>至于Bing的ChatGPT则坚称李强和习近平“没有关系”。虽然它边说没关系，边顺道提供了“李强将成习近平副手”的报道。</p><p>三：ChatGPT写出来的评论，编辑大概会退稿</p><p>虽然CNET有利用人工智能工具写作报道，但CNET写的都是“楼市这么低迷，我们还要买房子吗”这类的解释性短文，而评论组编辑雨欣却将难度大大提升：她尝试用ChatGPT来写我在2020年写的，探讨后极权下爱的意义的“这时代的爱与希望”系列。在ChatGPT交出来的文章中，这是写得比较好的部份：</p><p>
</p><p>“……在这个社会中，人们或许会面临生存和自由的限制，但是他们依然渴望爱和希望。虽然这些价值观可能会受到威胁和限制，但人们的渴望和追求并没有减少。在这样的时代，爱和希望可以扮演非常重要的角色，成为人们心中的明灯，指引他们前进。</p>
<p>首先，爱在这样的社会中依然有著无限的价值。虽然政府或许会监视人民的行动，但是他们无法监视人们心中的情感。人们依然可以在他们的生活中发现爱，并且透过爱来抵抗压迫和限制。爱可以是夫妻之间的感情、家人之间的关系，也可以是朋友之间的友谊，甚至是陌生人之间的互助。在这样的社会中，爱成为人们唯一可以自由表达和维护的价值观之一。”</p>
<p></p><p>我问雨欣：“作为编辑，收到这份稿件会有甚么感想？”</p><p>“如果有作者写了这么一篇稿的话，我会让作者把‘无法被监察的情感’的部份写得细一点，因为其实这个想法是可以的，但现在写得太笼统太模糊了。如果这一部份可以再写得有理有逻辑一点，‘厚实’一点，可能还有救……文章现在写得四平八稳，四平八稳不会错，但不会令我想反复细看。”</p><p>“比较大问题的是，这篇文章看不出作者性。一篇好的评论不外乎两方面：第一要有作者性，即是对问题的独特论述过程。论述过程可能包括作者自己的研究﹑专业领域的知识和经历。然后作者要有和他人不同的观点和看法，才是能够成立的评论。第二个方面就是文笔。评论有很多风格，可以毒辣可以感人，当然也能像写论文那样。但就算是写成论文那样，我都会希望这篇可以调整，要多一点个人的思考，要有些个人的‘东西’在里面……”</p><p>“所以这篇就，不好意思，应该要退稿……”</p><p><a class="image" href="https://theinitium.com/article/20230309-the-techsmiths-is-ai-replacing-journalists/undefined"><img src="https://d32kak7w9u5ewj.cloudfront.net/media/image/2023/03/75476abcd5884fa1b8f0f7a8412d0c82.jpeg?imageView2/1/w/1080/h/721/format/jpg" alt="2015年6月17日，香港，苹果日报印刷厂内，摆放著两卷用来印刷的纸。" itemprop="contentUrl" referrerpolicy="no-referrer"></a>
</p><figcaption itemprop="caption">2015年6月17日，香港，苹果日报印刷厂内，摆放著两卷用来印刷的纸。<span class="credit">摄：Xaume Olleros/Bloomberg via Getty Images</span></figcaption><p></p><p>我跟两位专家讨论实验结果</p><p>我觉得自己需要专家来跟我一起解读这些实验结果，于是我用ChatGPT写了封约访电邮，改了改之后发给了研究聊天机械人（Chatbot）和自然语言处理（natural language processing；NLP）的机械学习专家Rohan Kumar Yadav。相信他应该看不出来我是用ChatGPT写的约访信，又或者不太介意我的机械约访，因为他立刻就答应了跟我聊聊ChatGPT。</p><p>我跟Rohan说，ChatGPT很爱给假资料﹑制造假网址；而我们问ChatGPT两个政治人物有甚么关系时，它也给出了一大段写得很流畅，骤眼看非常可靠的资料，但几乎全部都是编的。它究竟是怎样写出看起来像人话的废话的？Rohan笑笑说：“首先我们要知道，ChatGPT﹑GPT-3﹑Google的LambDA这一类大型语言模型，设计的用意都是学习语言本身，而不是获取或理解资讯。所以它都是以‘统计学上合理’的方式写作的，即是写了一个字之后再预测下一个，看看最有可能是甚么字。接收到你的问题之后，它的神经网络就会开始一个计算过程：基于网路上几十亿个页面，最符合预期的答法是甚么？”</p><p>“所以你问的‘政治人物A和B之间的关系’，ChatGPT的答案就是它在统计了几百﹑几千亿个字后，在统计学上最合理的答案。但问题是，如果它读过的这些资料根本没有讲A和B之间有甚么关系呢？人类可能会懂得判断：‘哦，他们可能根本不认识对方’，但ChatGPT是不懂得判断的。它会做的只是不断地用统计学来预测下一个字。所以它就按著它读过的资料，找出那些讨论政坛关系的文章，再计算出一个符合语言规则的答案。这就是那些废话看起来那么真实的原因。”</p><p>“而又因为ChatGPT没有理解能力，它有时会给出不知所云的答案，像出现幻觉（hallucinations）一样。电脑科学上这种模型被称为‘随机鹦鹉’（stochastic parrots）。”Rohan说。“<a href="https://dl.acm.org/doi/10.1145/3442188.3445922">随机鹦鹉</a>”即只会制造语法上合理的字串或语句，但并不真的理解文字的意义的演算法或模型。这周四才加入ChatGPT讨论的语言学大师杭姆斯基（Noam Chomsky）也在《纽时》的<a href="https://www.nytimes.com/2023/03/08/opinion/noam-chomsky-chatgpt-ai.html">文章</a>解释了ChatGPT为何没法理解自然语言的语境和逻辑，并呼吁大众不要被要被模型的表面流畅度迷惑。</p><p>“与其要它生成甚么，我会倾向给它已有的文字让它顺一下，或者让它总结一些文章，那些它都做得很好。但要它写作就是另一回事了。”我们也发现ChatGPT很会理顺文句，也很会整理资料。有些套路﹑模板式﹑不要求独特性的写作任务，例如约访电邮或公式书信，的确可以大部份交给ChatGPT处理。</p><p>那大概就是代表ChatGPT根本不可能有创意，所以写出来的东西才那么套路又无聊？端的撰稿人和机械学习工程师朱孝文（Lanston）对此有点保留。“要看你怎样定义创意吧？例如毕加索看了梵高的画，却可以得到灵感开创跟梵高完全不同的新流派，这很明显是创意，ChatGPT﹑<a href="https://theinitium.com/article/20221024-technology-can-artificial-intelligence-have-consciousness/">Midjourney</a>等工具大概做不到。但你要ChatGPT用马克吐温的文字风格写科幻小说，或者要Midjourney用梵高的风格画伊藤润二的恐怖主题，只要训练数据足够，这些工具很轻易就做得到，对我们来说却相对困难。这大概也可以叫创意？”</p><p>“与其说它‘完全没有创意’，不如说问题是ChatGPT的创意非常依赖已经存在的维度，它没有突破的能力。我们可以对比围棋人工智能AlphaGO－－为甚么它就可以‘突破’，可以行出人类意想不到的创新步法？问题很可能在于围棋跟艺术﹑文学的本质不同。围棋有很明确的目标，就是在终局的时候，贴目后目数比对方多。所以它的演算法不会抗拒‘创新’的步法。甚至乎<a href="https://theinitium.com/article/20171019-evening-brief/">AlphaGo Zero</a>已经不需要依赖人工“喂”棋谱做训练数据，它左右互搏已经足够进步。棋下多了有人类没下过的步法就不足为奇。但写作不同。以端的深度文章为例，虽然我们都可以笼统地说‘写得好’代表写得流畅﹑有深度﹑有内涵﹑有独特见解……但这些全部都不是能轻易量化的明确目标。”</p><p>“所以我们只能依赖已经存在的文章来训练ChatGPT，而它的演算法目的就是从现有文章中计算出下一个字的机会率。所以<strong>有人说过的东西它才会说，而很多人说的东西它更有机会说</strong>。就像你的同事说的：‘为甚么ChatGPT写的东西如此像宣传机器，空洞且含情脉脉？’因为它本身的目的就是讲老生常谈。”美国科幻作家姜峰楠（Ted Chiang）二月初在《纽约客》<a href="https://www.newyorker.com/tech/annals-of-technology/chatgpt-is-a-blurry-jpeg-of-the-web">撰文</a>，也指出我们就算有办法阻止ChatGPT编造事实，也不应该用它来再制造更多网上内容，因为它的运作原理，就是将网上本来就有的东西打包再吐出来。姜一针见血：“本来就有很多公司这么做了，这些公司叫内容农场。”</p><p>我问Lanston：“如果要在这么不明确的目标下实现‘突破’的创意，好像要强人工智能才做得到？”强人工智能（strong AI或AGI）的定义仍然有争议，但一般指能像人类一样思考和处理不同类型问题的人工智慧。相对的是弱人工智能（weak AI），即只能处理特定问题的人工智能。现存的所有演算法或人工智能工具都是弱人工智能，而强人工智能不止仍未出现，连是否有可能出现都还很有争议。</p><p>“很难说。首先我们仍不知道强人工智能是否可能或如何可能。其次就是，就算不是强人工智能也有可能有创意吧？举个例，海豚﹑八爪鱼﹑大象和狗肯定都不及人类聪明，但那代表牠们没有创意吗？好像很难这么说。”</p><p><a class="image" href="https://theinitium.com/article/20230309-the-techsmiths-is-ai-replacing-journalists/undefined"><img src="https://d32kak7w9u5ewj.cloudfront.net/media/image/2023/03/8a6c9f1c736b440b846cf554b9f8c681.jpg?imageView2/1/w/1080/h/742/format/jpg" alt="记者在一家报纸的编辑部工作。" itemprop="contentUrl" referrerpolicy="no-referrer"></a>
</p><figcaption itemprop="caption">记者在一家报纸的编辑部工作。<span class="credit">DreamStudio 生成</span></figcaption><p></p><p>但ChatGPT这种大型语言模型是通往强人工智能的路吗？对此Rohan和Lanston有完全不同的看法。Rohan认为，五年或十年前，我们完全想像不到会有像ChatGPT这样强大甚至通用的工具出现，既可以做情绪分析，又可以很好地掌握语言﹑能理解相对含蓄的语义问题，同时可以作为搜索引擎使用。“我很可能是错的，但我认为，在短时间内人工智能工具已经得到这么巨大的进步，我们或者真的能在这条路上发展出通用人工智能。”</p><p>而Lanston则不太同意这个说法。“就算退一万步讲，只处理文字的ChatGPT能够发展出强人工智能，那也是个很没效率的方法。如果要达成能跟人类相比的强人工智能，就必须有效率地整合不同类型的数据。世上的生物有几多种是只有听觉而没有触觉和视觉的？例如我至少要有视觉或触觉去帮助我获得空间认知，不然只是‘水杯’这种简单的概念我也理解不了。所以，如果只靠序列式数据（sequencing data）就可以有效率理解整个世界，那么地球生物的触觉和视觉都可以退化，只剩一只超敏感的耳朵就够了。”</p><p>我有点疑惑。“但那好像有点暗示了，因为我们进化成了这样子，这样就是高等智慧唯一应该有的样子。”</p><p>“但这就是我们一直追求的‘强人工智能’的定义。我们自己就是自己见识过最聪明的生物，除了人类本身，我们似乎没有更好的模仿对象了。”</p><p>谁在害怕“人工智能”？</p><p>实验结果很明显了：第一，端编辑室应该还没有谁能被ChatGPT“取代”；第二，未来的人工智能会不会“取代”新闻人还是未知之数。不过还有个问题没有解决：如果大型语言模型离“取代”编辑记者遥遥无期，甚至暂时不算有太多实用价值，为甚么社会讨论“人工智能”时，总是用“取代人类”的叙事？这种自制的生存威胁是怎么来的？</p><p>我们对于人工智能的想像多来自荷里活电影：1968年寇比力克的经典《2001太空漫游》（2001 Space Odyssey）中的HAL因害怕“被关掉”而将太空船上的人杀死；1979年《异形》（Alien）的“仿生人”为了将异形运回地球而想杀死主角，2004年改编自科幻大师阿西莫夫小说的《智能叛变》（I, Robot）讲机械警察人要推倒人类社会。女性机械人或人工智能成为“femme fatale”（影视中的蛇蝎美人典型）的电影就有近年的《智能叛侣》（Ex-Machina）和去年上映的《人工杀姬》（M3GAN）等。当然也有些电影中的“人工智能”不是要杀人（或在杀人的路上），但它们仍被塑造成有感知力的：例如2001年史匹堡的电影“A.I.人工智慧”（A.I. Artificial Intelligence）的主角就是一个渴望爱的机械小男孩。</p><p>主流娱乐影视和科幻小说令我们将“人工智能”和能思想﹑有感情的机械人或电脑联想在一起，但正如上面说的，强人工智能是否真的可能还是未知数，而所有现有的“人工智能工具”都是只能做特定任务的弱人工智能。“人工智能”其实是个总称，是电脑科学下专门模拟人类智能的分支；近年很热门的机械学习就是人工智能的一种，目的是训练机械从数据中学习﹑识别模式并在几乎不需人为干预的情况下作出后续判断。在主流媒体，无论是甚么工具都一律被称为“人工智能”，但其实专家们一直在呼吁媒体区分演算法（algorithm）﹑程式（program）﹑自动化（automation）等等科技名词，以免加深大众对人工智能的误解。但为了搜索引擎优化（SEO），很多媒体仍然将任何工具都称为人工智能。无奈地，这篇文章也不例外：如果标题写的是“大型语言模型实验”，点击率会不会少了一大截？</p><p>对于大众对人工智能将会变得无所不能，早晚要超越及“取代”人类的想法，纽约大学公共利益技术联盟的研究主任Meredith Broussard 提出了“科技沙文主义”（technochauvinism）的概念：“<strong>当你预测一种新的技术工具要开创一个新时代时，问问自己：真的会吗？</strong> 对技术工具的夸大其词是一种偏见，一种我称之为‘科技沙文主义’的偏见。这种偏见假设了技术解决方案总是优于其他的解决方案。”她指出，我们现在对人工智能的恐惧，其实以前就出现过很多次，而之前“威胁”新闻的是微型电脑﹑桌面排版﹑智能手机﹑数据库﹑社交媒体﹑Youtube和其他串流媒体……而现在是“人工智能”。</p><p><a class="image" href="https://theinitium.com/article/20230309-the-techsmiths-is-ai-replacing-journalists/undefined"><img src="https://d32kak7w9u5ewj.cloudfront.net/media/image/2023/03/a78456393f904c04b54f570dbfda453c.jpeg?imageView2/1/w/1080/h/720/format/jpg" alt="2015年11月24日，中国北京，一座机械臂在用毛笔写中文字。" itemprop="contentUrl" referrerpolicy="no-referrer"></a>
</p><figcaption itemprop="caption">2015年11月24日，中国北京，一座机械臂在用毛笔写中文字。<span class="credit">摄：VCG via Getty Images</span></figcaption><p></p><p>西北大学新闻学院和电脑科学院教授Nick Diakopoulous也指出，人工智能毫无疑问将会改变编辑室的工作流程，但这个流程之前就被摄影﹑后制和电脑技术改变过很多次了。与其说是“取代”，人工智能更可以做的其实是“协助”：现时编辑室有很多工作是可以交付给科技的，例如我和身在挪威的Rohan在Google Meet做完访问，等了半小时Gmail邮箱就收到了访问的逐字稿，省却了以往访问后听录音做笔录的时间。Diakopoulous甚至说，人工智能科技更可能会制造新的工作岗位：知识管理﹑参数化﹑数据生产﹑模板编写﹑建立和操作自动生产系统，都是编辑室有可能围绕人工智能工具而产生的工作。</p><p>电脑科学家Ian Bogost在《大西洋》（The Atlantic）<a href="https://www.theatlantic.com/technology/archive/2023/02/chatgpt-ai-detector-machine-learning-technology-bureaucracy/672927/">撰文</a>，也指出ChatGPT的不足会带来更多工作：“无论ChatGPT和其他人工智能工具最终做了甚么，它们都会在进行所谓省力工作所需的劳动力上，施加新的劳动和管理制度。”结果就是为人类增加更多工种。无论这种“科技解决主义”（technological solutionism）是好是坏，人工智能工具和人类的关系，似乎都不是前者完全排挤后者。</p><p>工具人与新闻的未来</p><p>ChatGPT也许永远不会“取代”人类编辑和记者，但我们不能否认，甚至应该庆幸的是：这些工具会不断地进化，而且进化的速度是爆炸性的。只是十年前，我们已经几乎不能想像有ChatGPT那么会运用人类语言的工具，也没法想像人工智能可以画画﹑处理照片﹑作曲﹑下棋﹑写小说﹑协助科研﹑写程式﹑分析股市。而在社会还在为ChatGPT兴奋莫名的时候，Notion﹑Facebook都宣布了推出自己的人工智能工具，而微软也宣布将会推出“多模态大型语言模型”（multimodal large language model；MLLM）Kosmos-1，除了能处理语言还能分析图像。</p><p>“那新闻的未来是怎样的？”我问伦敦政经学院（LSE）Polis智库旗下JournalismAI项目的经理Mattia Paretti，“今天ChatGPT当然没有‘取代’记者，但十年后呢？这些工具变强的速度是爆炸性的，但十年后的人类却绝不会进化成超级人类。我们知道‘取代’是夸张了，但我们没法避免被科技改变。作为新闻人应该如何和人工智能工具相处？”</p><p>Mattia说：“其实看看历史就知道，新闻行业被科技‘挑战’的历史很长。可能人工智能的确跟之前的挑战不太一样，或者它带来的改变更巨大，但能比数十年前互联网出现时巨大吗？好像也很难说。但我会说新闻人－－应该是所有人－－都应该去理解它的长处和不足。我不是说大家都要去学编程，要懂得里面的数学甚么的。但我们需要能客观地谈论它，不需要夸大它们的作用和影响，更不需要有不必要的恐惧。或者到我们客观看待人工智能时，就会发现它会带来很多特别的机会。”</p><p>BBC多年前就根据一个牛津大学的研究，做了一个叫“机械人会不会抢了你的工作”的<a href="https://www.bbc.co.uk/news/technology-34066941">网站</a>，可以查各项职业的自动化风险（automation risk）。我试试键入自己的各项身份：“社会人文科学家”的自动化风险只有10％，非常低。“作家和翻译人员”的自动化机会率是三成多，仍然偏低；“记者和编辑”是8%。最近对考古学和古生物学很有兴趣，所以我也查了一下考古学家的自动化风险：低于1%。相比之下，零售业是重灾区，很多职业的自动化指数都超过九成。</p><p>这些数字应该让我们觉得安心吗？是不是因为社会学太困难太复杂了，所以机械人难以取代？也许－－但更有可能的是，没有人会去制造一个取代社会学家的机械人，因为社会学家这种行业本来就不太赚钱。不过社科学者起码还能在智库等机构工作，甚至现在许多做数据的社科学者会转行科技行业；但考古学家就不一样了，除了学院，他们需要十几年时间来学习的艰涩知识几乎没有职业化的机会。而高等教育现时面对的学术工作零散化﹑缺乏劳工保障﹑研究院里的剥削等问题，全部都不是由科技引起的，“自动化”相比学术工作者面对的种种问题实在微不足道。</p><p>传媒也一样。自动化会如何撼动新闻行业当然值得谈，但要数新闻行业的难处，甚么时候才会数到“被人工智能取代”呢？</p><p>这可能就是Broussard提出“科技沙文主义”的原因了：我们迷恋工具同时恐惧工具，忘记了科技既不会为我们创造乌托邦，大概率也不会为我们带来世界末日；科技能为我们做的很多，但不能为我们做的更多。Broussard也写道：“技术系统不会消除社会问题，但会转移和掩盖它们。”虽然这个结论可能老套并有点令人失望，但世界明天会变成怎样，这些工具最终会为我们的生活带来甚么改变，答案自有人类社会之始都没有改变过：就是人类本身。</p><p><small>（感谢实习记者徐凯鸣Stacey对本文的贡献。也感谢端传媒的同事们和我一起进行这场小实验，你们都是无法被取代的优秀新闻人。）</small></p>
