---
layout: post
title: Machine Learning - 資料前處理 Data Preprocessing - 機器學習筆記 - 如何刪除低方差 的特徵? 如何自行定義閾值?
  - Sklearn VarianceThreshold套件 - 使用教學...
date: 2021-05-19 05:39:56.000000000 +00:00
link: https://matters.news/@CHWang/machine-learning-%25E8%25B3%2587%25E6%2596%2599%25E5%2589%258D%25E8%2599%2595%25E7%2590%2586-data-preprocessing-%25E6%25A9%259F%25E5%2599%25A8%25E5%25AD%25B8%25E7%25BF%2592%25E7%25AD%2586%25E8%25A8%2598-%25E5%25A6%2582%25E4%25BD%2595%25E5%2588%25AA%25E9%2599%25A4%25E4%25BD%258E%25E6%2596%25B9%25E5%25B7%25AE-%25E7%259A%2584%25E7%2589%25B9%25E5%25BE%25B5-%25E5%25A6%2582%25E4%25BD%2595%25E8%2587%25AA%25E8%25A1%258C%25E5%25AE%259A%25E7%25BE%25A9%25E9%2596%25BE%25E5%2580%25BC-sklearn-variance-threshold%25E5%25A5%2597%25E4%25BB%25B6-%25E4%25BD%25BF%25E7%2594%25A8%25E6%2595%2599%25E5%25AD%25B8%25E7%25AD%2586%25E8%25A8%2598-bafyreihoynvdm4sgrexevf4jlloxwnc5ptqb6ydj7oouzo2mnhup5y66ya
categories: matters
tags: blog
author: 為自己Coding
---

<h1><a href="https://github.com/chwang12341/Machine-Learning/tree/master/VarianceThreshold" target="_blank">Github連結</a></h1><figure class="image">
      <picture>
        <source type="image/webp" media="(min-width: 768px)" srcset="https://assets.matters.news/processed/1080w/embed/af65ab32-9b13-4be2-ba75-4f6e4591a795.webp" onerror="this.srcset='https://assets.matters.news/embed/af65ab32-9b13-4be2-ba75-4f6e4591a795.jpeg'">

        <source media="(min-width: 768px)" srcset="https://assets.matters.news/processed/1080w/embed/af65ab32-9b13-4be2-ba75-4f6e4591a795.jpeg" onerror="this.srcset='https://assets.matters.news/embed/af65ab32-9b13-4be2-ba75-4f6e4591a795.jpeg'">

        <source type="image/webp" srcset="https://assets.matters.news/processed/540w/embed/af65ab32-9b13-4be2-ba75-4f6e4591a795.webp">

        <img src="https://assets.matters.news/embed/af65ab32-9b13-4be2-ba75-4f6e4591a795.jpeg" srcset="https://assets.matters.news/processed/540w/embed/af65ab32-9b13-4be2-ba75-4f6e4591a795.jpeg" loading="lazy" referrerpolicy="no-referrer">
      </picture>
    <figcaption><span>攝影師：Michael Block，連結：Pexels</span></figcaption></figure><p><br></p><p><br></p><h2><strong>1. 為什麼要刪除低方差的特徵?</strong></h2><p>這是因為當有個特徵裡面的值有很多是一樣的時候，大家還會認為這個特徵作用很大嗎?假設我們今天有個特徵裡面只包含0和1的值，而0佔了這個特徵95%的數量，也就是特徵中有95%的值是一樣的，那很明顯這個特徵對我們要進行分析結果的作用不大，更不用說如果全部都是0那這個特徵就根本沒意義了</p><p><br></p><p><strong>方差公式</strong></p><figure class="image">
      <picture>
        <source type="image/webp" media="(min-width: 768px)" srcset="https://assets.matters.news/processed/1080w/embed/12788a4c-52ef-496c-ac3e-6a5cf65706b5.webp" onerror="this.srcset='https://assets.matters.news/embed/12788a4c-52ef-496c-ac3e-6a5cf65706b5.png'">

        <source media="(min-width: 768px)" srcset="https://assets.matters.news/processed/1080w/embed/12788a4c-52ef-496c-ac3e-6a5cf65706b5.png" onerror="this.srcset='https://assets.matters.news/embed/12788a4c-52ef-496c-ac3e-6a5cf65706b5.png'">

        <source type="image/webp" srcset="https://assets.matters.news/processed/540w/embed/12788a4c-52ef-496c-ac3e-6a5cf65706b5.webp">

        <img src="https://assets.matters.news/embed/12788a4c-52ef-496c-ac3e-6a5cf65706b5.png" srcset="https://assets.matters.news/processed/540w/embed/12788a4c-52ef-496c-ac3e-6a5cf65706b5.png" loading="lazy" referrerpolicy="no-referrer">
      </picture>
    <figcaption><span></span></figcaption></figure><p><br></p><p><br></p><h2><strong>2. 使用時機?</strong></h2><p><br></p><p>特徵值為離散型的數據時，適合使用這個方法，如果為連續型，那最好將它轉成離散型再使用，可以幫助我們刪除掉那些值變化很小的特徵</p><h2><br></h2><h2><strong>3. VarianceThreshold 說明</strong></h2><p><br></p><ul><li><strong>說明:</strong> sklearn中feature_selection裡的VarianceThreshold是一種特徵選擇的方法，也是在做前處理時非常有用的功能</li><li><strong>功能:</strong> VarianceThreshold會幫助我們刪除掉那些方差未達到指定閾值的特徵，預設情況下，它會刪除掉所有方差為零的特徵，也就是值都為一樣的特徵</li></ul><h2><br></h2><h2><br></h2><h2><strong>4. 參數、屬性、方法介紹</strong></h2><p><br></p><p><strong>參數</strong></p><ul><li>threshold: 傳入浮點數，預設為0，當訓練集中低於這個閾值的特徵將被刪除，預設為保留方差不為零的特徵，也就是刪除所有值一樣的特徵</li></ul><p><strong>屬性</strong></p><ul><li>variances: 個體特徵的方差(差異)</li></ul><p><strong>方法</strong></p><ul><li>fit(X[y]):從X了解經驗方差</li><li>fit_transform(X[y]): 擬和數據，然後對其進行轉換</li><li>get_params([deep]): 獲取此估計量的參數</li><li>get_support([indices]): 獲取所選特徵的掩碼或整數索引</li><li>inverse_transform(X): 反向轉換操作</li><li>set_params(**params): 設置此估算器的參數</li><li>transform(X): 将X縮小為選定的特徵</li></ul><h2><br></h2><h2><br></h2><h2><strong>5. 實作</strong></h2><p><br></p><h3><strong>導入VarianceThreshold套件</strong></h3><pre class="ql-syntax">## VarianceThreshold
from sklearn.feature_selection import VarianceThreshold
</pre><h3><br></h3><h3><strong>預設方法: 刪除所有一樣值的特徵</strong></h3><pre class="ql-syntax">## 創建數據集
x = [[2, 8, 7, 9], [3, 8, 6, 8], [7, 8, 10, 10]]
print('Before: ', x)
selector = VarianceThreshold()
print('After:')
print(selector.fit_transform(x))
print('Variance: ', selector.variances_)
</pre><p><strong>執行結果</strong></p><pre class="ql-syntax">Before: [[2, 8, 7, 9], [3, 8, 6, 8], [7, 8, 10, 10]]
After:
[[ 2 7 9]
 [ 3 6 8]
 [ 7 10 10]]
Variance: [4.66666667 0.    2.88888889 0.66666667]
</pre><ul><li>原數據集的第二個特徵(列)值都為8，所以在结果中被拿掉了</li></ul><h3><br></h3><h3><strong>布林數據中，刪除0或1其中之一出現機率大於80%的特徵</strong></h3><ul><li>如果我們要刪除0或1其中之一出現機率大於80%的特徵，而布林特徵是伯努力隨機變量，所以方差為 Var[X] = p(1 - p)</li><li>閾值: 0.8*(1 - 0.8)</li></ul><pre class="ql-syntax">## 構建數據集
x= [[0, 1, 0, 1], [1, 0, 1, 1], [0, 0, 1, 1], [1, 0, 1, 0]]
print('Before: ', x)
selector = VarianceThreshold(threshold = (0.8 * (1 - 0.8)))
print('After:')
print (selector.fit_transform(x))
</pre><p><strong>執行結果</strong></p><pre class="ql-syntax">Before: [[0, 1, 0, 1], [1, 0, 1, 1], [0, 0, 1, 1], [1, 0, 1, 0]]
After:
[[0 1 0 1]
 [1 0 1 1]
 [0 0 1 1]
 [1 0 1 0]]
</pre><h3><br></h3><h3><strong>如果想將過濾訓練集的結果直接應用到測試集</strong></h3><pre class="ql-syntax">import pandas as pd
​
## 創建數據集 - 訓練集與測試集
a = [[2, 8, 7, 9], [3, 8, 6, 8], [7, 8, 10,10]]
b = [[3, 3, 7, 9], [4, 2, 6, 10], [8, 4, 2, 101]]
x_train = pd.DataFrame(a, columns = ['A','B', 'C', 'D'])
x_test = pd.DataFrame(b, columns = ['A','B', 'C', 'D'])
selector = VarianceThreshold()           
          
## 擬和訓練集，在轉換到測試集
selector.fit(x_train)
selector.transform(x_test)
</pre><p><strong>執行結果</strong></p><pre class="ql-syntax">array([[ 3, 7, 9],
   [ 4, 6, 10],
   [ 8, 2, 101]], dtype=int64)
</pre><ul><li>結果可以看出，在訓練集中第二個特徵被濾掉了，所以轉換到測試集的時候，儘管測試集的第二個特徵值沒有都一樣，還是被濾掉了</li></ul><h2><br></h2><p><br></p><h2><strong>6. 遇到一個大問題?特徵的名稱不見了!!怎麼保留特徵?如何反推原數據?</strong></h2><p><br></p><pre class="ql-syntax">## 創建數據集 - 訓練集與測試集
a = [[2, 8, 7, 9], [3, 8, 6, 8], [7, 8, 10, 10]]
b = [[3, 3, 7, 9], [4, 2, 6, 10], [8, 4, 2, 10]]
x_train = pd.DataFrame(a, columns = [ 'A', 'B', 'C', 'D'])
x_test = pd.DataFrame(b, columns = ['A', 'B', 'C', 'D'])
selector = VarianceThreshold()
​
## 擬和訓練集，在轉換到測試集中
selector.fit(x_train)
print('No Features Name: ')
print(selector.transform(x_test))
​
## 保留特徵名稱
features_name = x_train.columns.values.tolist()
print('original Features: ', features_name)
​
## 被留下來的特徵索引
preserve_feature_index = selector.get_support(indices = True)
​
print('Preserve Features Index: ', preserve_feature_index)
result_features = []
for i in preserve_feature_index:
  result_features.append(features_name[i])
​
print('Final Features: ', result_features)
​
## 组合成最終結果
pd.DataFrame(selector.transform(x_test), columns = result_features)
</pre><p><strong>執行結果</strong></p><figure class="image">
      <picture>
        <source type="image/webp" media="(min-width: 768px)" srcset="https://assets.matters.news/processed/1080w/embed/35c24445-3e61-47e5-aa43-8df1e5bb2220.webp" onerror="this.srcset='https://assets.matters.news/embed/35c24445-3e61-47e5-aa43-8df1e5bb2220.png'">

        <source media="(min-width: 768px)" srcset="https://assets.matters.news/processed/1080w/embed/35c24445-3e61-47e5-aa43-8df1e5bb2220.png" onerror="this.srcset='https://assets.matters.news/embed/35c24445-3e61-47e5-aa43-8df1e5bb2220.png'">

        <source type="image/webp" srcset="https://assets.matters.news/processed/540w/embed/35c24445-3e61-47e5-aa43-8df1e5bb2220.webp">

        <img src="https://assets.matters.news/embed/35c24445-3e61-47e5-aa43-8df1e5bb2220.png" srcset="https://assets.matters.news/processed/540w/embed/35c24445-3e61-47e5-aa43-8df1e5bb2220.png" loading="lazy" referrerpolicy="no-referrer">
      </picture>
    <figcaption><span></span></figcaption></figure><p><br></p><p><br></p><h2><br></h2><h2><strong>Reference</strong></h2><p><a href="https://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.VarianceThreshold.html#sklearn.feature_selection.VarianceThreshold.get_support" target="_blank">https://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.VarianceThreshold.html#sklearn.feature_selection.VarianceThreshold.get_support</a></p>
